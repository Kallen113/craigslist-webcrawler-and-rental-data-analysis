{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given bugs associated with older versions of the webcrawler (*fixed on Jan 1, 2022*) , we need to clean and transform data that were scraped prior to this: namely, October 27th-Dec 31st, 2021 data.\n",
    "\n",
    "#### What was this bug?: Previously, the webcrawler did not correctly handle rental listings whose listings were deleted or ended while the webcrawler was accessing each individual rental listing.\n",
    "\n",
    "#### NB: Namely, the bug would cause a misalignment issue in which scraped rental listings data were not properly aligned with their 'true' listing URLs and listing ids (ie, what they actually should have been). As a result, some of the data are not lined up properly, which would lead to misleading analysis and much more noise in the dataset. Consequently, we need to remove all misaligned data, but save/salvage all of the rental listings data records that line up properly along the given rows for each column.\n",
    "\n",
    "## How do we correctly keep only the accurate rental listings data, among these Oct to Dec 2021 CSV files?\n",
    "\n",
    "## First, we need to recursively load in all of these 2021 scraped CSV files.\n",
    "\n",
    "## 2.) Next, in order to filter out any misaligned data, we need to compare the 'true' listing ids for each given record of data, and compare this with the listing id data that the webcrawler scraped.\n",
    "\n",
    "## To verify rental listings data that are accurate and not negatively affected by the old misalignment bug,, we need to use a regex pattern to look up the 'true'\n",
    "\n",
    "### This regex will search for 10 digits (ie, 0-10) in a row since a.) listing ids will *always* be contained within a rental listing URL--and we have previously determined the webcrawler has *never* had any problems in parsing the URL listings correctly. b) In addition, each listing ID is always a 10-digit unique ID.\n",
    "\n",
    "## 3.) We then need to add 2 columns--'flat' & 'land'-- which were added to the webcrawler program with some additional changes in Jan 2022. \n",
    "\n",
    "## 4) Finally, save the cleaned and transformed data as a single CSV in the new 'old_scraped_data' folder, within the scraped_data> sfbay folders within the webcrawler project's CraigslistWebScraper folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Import relevant data (after library imports):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports-- file processing\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# data analysis libraries & SQL libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23502 entries, 0 to 23501\n",
      "Data columns (total 47 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   listing_urls             23502 non-null  object \n",
      " 1   ids                      23460 non-null  float64\n",
      " 2   sqft                     18082 non-null  float64\n",
      " 3   cities                   23449 non-null  object \n",
      " 4   prices                   23448 non-null  object \n",
      " 5   bedrooms                 23118 non-null  float64\n",
      " 6   bathrooms                23404 non-null  object \n",
      " 7   attr_vars                23440 non-null  object \n",
      " 8   listing_descrip          23440 non-null  object \n",
      " 9   date_of_webcrawler       23470 non-null  object \n",
      " 10  kitchen                  23450 non-null  float64\n",
      " 11  date_posted              23440 non-null  object \n",
      " 12  region                   23502 non-null  object \n",
      " 13  sub_region               23502 non-null  object \n",
      " 14  cats_OK                  23502 non-null  int64  \n",
      " 15  dogs_OK                  23502 non-null  int64  \n",
      " 16  wheelchair_accessible    23502 non-null  int64  \n",
      " 17  laundry_in_bldg          23502 non-null  int64  \n",
      " 18  no_laundry               23502 non-null  int64  \n",
      " 19  washer_and_dryer         23502 non-null  int64  \n",
      " 20  washer_and_dryer_hookup  23502 non-null  int64  \n",
      " 21  laundry_on_site          23502 non-null  int64  \n",
      " 22  full_kitchen             23502 non-null  int64  \n",
      " 23  dishwasher               23502 non-null  int64  \n",
      " 24  refrigerator             23502 non-null  int64  \n",
      " 25  oven                     23502 non-null  int64  \n",
      " 26  flooring_carpet          23502 non-null  int64  \n",
      " 27  flooring_wood            23502 non-null  int64  \n",
      " 28  flooring_tile            23502 non-null  int64  \n",
      " 29  flooring_hardwood        23502 non-null  int64  \n",
      " 30  flooring_other           23502 non-null  int64  \n",
      " 31  apt_type                 23502 non-null  int64  \n",
      " 32  in_law_apt_type          23502 non-null  int64  \n",
      " 33  condo_type               23502 non-null  int64  \n",
      " 34  townhouse_type           23502 non-null  int64  \n",
      " 35  cottage_or_cabin_type    23502 non-null  int64  \n",
      " 36  single_fam_type          23502 non-null  int64  \n",
      " 37  duplex_type              23502 non-null  int64  \n",
      " 38  is_furnished             23502 non-null  int64  \n",
      " 39  attached_garage          23502 non-null  int64  \n",
      " 40  detached_garage          23502 non-null  int64  \n",
      " 41  carport                  23502 non-null  int64  \n",
      " 42  off_street_parking       23502 non-null  int64  \n",
      " 43  no_parking               23502 non-null  int64  \n",
      " 44  EV_charging              23502 non-null  int64  \n",
      " 45  air_condition            23502 non-null  int64  \n",
      " 46  no_smoking               23502 non-null  int64  \n",
      "dtypes: float64(4), int64(33), object(10)\n",
      "memory usage: 7.5+ MB\n",
      "Sanity check--overview (ie, via .info() method) of the imported scraped data data:: None\n"
     ]
    }
   ],
   "source": [
    "## import old scraped data, which needs to be cleaned  \n",
    "def recursively_import_all_CSV_and_concat_to_single_df(parent_direc, fn_regex=r'*.csv'):\n",
    "    \"\"\"Recursively search parent directory, and look up all CSV files.\n",
    "    Then, import all CSV files and concatenate into a single Pandas' df using pd.concat()\"\"\"\n",
    "    path =  parent_direc # specify parent path of directories containing the scraped rental listings CSV data -- NB: use raw text--as in r'path...', or can we use the double-back slashes to escape back-slashes??\n",
    "    df_concat = pd.concat((pd.read_csv(file) for file in glob.iglob(\n",
    "        os.path.join(path, '**', fn_regex), \n",
    "        recursive=True)), ignore_index=True\n",
    "        )  # os.path.join helps ensure this concatenation is OS independent\n",
    "\n",
    "    return df_concat\n",
    "\n",
    "\n",
    "## specify directory and import data\n",
    "scraped_data_path = r\"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\old_scraped_data\"\n",
    "# import data\n",
    "df = recursively_import_all_CSV_and_concat_to_single_df(scraped_data_path)\n",
    "print(f\"Sanity check--overview (ie, via .info() method) of the imported scraped data data:: {df.info()}\") # sanity check-examine size of dataset, columns, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, clean data, with the following steps in mind:\n",
    "\n",
    "## Clean data based on 3 main things:\n",
    "\n",
    "###  1.) deduplicate data based on listing ids\n",
    "\n",
    "### 2.) Remove misaligned data by:\n",
    "### --firstly a.) using a regex pattern on the listing_urls (ie, rental listing urls)\n",
    "### to check for the 'true' listing ids.\n",
    "### Let's call this 'true_listing_ids'. \n",
    "\n",
    "\"\"\"-- *NB*: given that listing ids are always 10 digits, a regex that can parse the listing ids is\n",
    "as follows:\n",
    "<<<\n",
    "regex_pattern = r\"[0-9]{10}\"   # search for any series of 10 consecutive digits \n",
    "\"\"\"\n",
    "### --then: b.) identifying any rows in which the scraped ids column differ (ie, are not exactly equal to) \n",
    "### the 'true' listing ids. \n",
    "\n",
    "### 3.) Rename existing cols as needed,  and *add* and parse 3 or 4 additional dummy variable cols such as 'flat' & 'land'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Deduplicate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) deduplicate\n",
    "def deduplicate_df(df):\n",
    "    \"\"\"Remove duplicate rows based on listing ids\"\"\"\n",
    "    return df.drop_duplicates(keep='first', subset = ['ids'])\n",
    "\n",
    "df = deduplicate_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18440 entries, 0 to 23501\n",
      "Data columns (total 47 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   listing_urls             18440 non-null  object \n",
      " 1   ids                      18439 non-null  float64\n",
      " 2   sqft                     14433 non-null  float64\n",
      " 3   cities                   18437 non-null  object \n",
      " 4   prices                   18435 non-null  object \n",
      " 5   bedrooms                 18245 non-null  float64\n",
      " 6   bathrooms                18435 non-null  object \n",
      " 7   attr_vars                18436 non-null  object \n",
      " 8   listing_descrip          18436 non-null  object \n",
      " 9   date_of_webcrawler       18440 non-null  object \n",
      " 10  kitchen                  18438 non-null  float64\n",
      " 11  date_posted              18436 non-null  object \n",
      " 12  region                   18440 non-null  object \n",
      " 13  sub_region               18440 non-null  object \n",
      " 14  cats_OK                  18440 non-null  int64  \n",
      " 15  dogs_OK                  18440 non-null  int64  \n",
      " 16  wheelchair_accessible    18440 non-null  int64  \n",
      " 17  laundry_in_bldg          18440 non-null  int64  \n",
      " 18  no_laundry               18440 non-null  int64  \n",
      " 19  washer_and_dryer         18440 non-null  int64  \n",
      " 20  washer_and_dryer_hookup  18440 non-null  int64  \n",
      " 21  laundry_on_site          18440 non-null  int64  \n",
      " 22  full_kitchen             18440 non-null  int64  \n",
      " 23  dishwasher               18440 non-null  int64  \n",
      " 24  refrigerator             18440 non-null  int64  \n",
      " 25  oven                     18440 non-null  int64  \n",
      " 26  flooring_carpet          18440 non-null  int64  \n",
      " 27  flooring_wood            18440 non-null  int64  \n",
      " 28  flooring_tile            18440 non-null  int64  \n",
      " 29  flooring_hardwood        18440 non-null  int64  \n",
      " 30  flooring_other           18440 non-null  int64  \n",
      " 31  apt_type                 18440 non-null  int64  \n",
      " 32  in_law_apt_type          18440 non-null  int64  \n",
      " 33  condo_type               18440 non-null  int64  \n",
      " 34  townhouse_type           18440 non-null  int64  \n",
      " 35  cottage_or_cabin_type    18440 non-null  int64  \n",
      " 36  single_fam_type          18440 non-null  int64  \n",
      " 37  duplex_type              18440 non-null  int64  \n",
      " 38  is_furnished             18440 non-null  int64  \n",
      " 39  attached_garage          18440 non-null  int64  \n",
      " 40  detached_garage          18440 non-null  int64  \n",
      " 41  carport                  18440 non-null  int64  \n",
      " 42  off_street_parking       18440 non-null  int64  \n",
      " 43  no_parking               18440 non-null  int64  \n",
      " 44  EV_charging              18440 non-null  int64  \n",
      " 45  air_condition            18440 non-null  int64  \n",
      " 46  no_smoking               18440 non-null  int64  \n",
      "dtypes: float64(4), int64(33), object(10)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Remove misaligned data\n",
    "\n",
    "### 2 a) Use regex to parse the 'true' listing ids: \n",
    "### ; b) transform data types of ids to match the object data type of the 'true' listing ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True listing ids:\n",
      "0        7401005952\n",
      "1        7401007644\n",
      "2        7401007110\n",
      "3        7400999282\n",
      "4        7397957717\n",
      "            ...    \n",
      "23495    7423968538\n",
      "23496    7423960763\n",
      "23498    7414126130\n",
      "23500    7421288799\n",
      "23501    7423920882\n",
      "Name: true_listing_ids, Length: 18440, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2.) Remove misaligned data:\n",
    "\n",
    "# 2 a) Use regex to parse the 'true' listing ids:\n",
    "def regex_create_pandas_col(df, col, regex_pattern):\n",
    "    \"\"\"Create new series for DataFrame based on regex pattern results, given DataFrame (df) and Series (ie, col). \"\"\"\n",
    "    return df[col].str.extract(regex_pattern) # apply regex pattern to parse data from col and create new column\n",
    "\n",
    "\n",
    "# specify regex pattern to parse the 'true' listing ids from the listing urls:\n",
    "regex_pattern = r\"([0-9]{10})\"  # *NB*: wrap regex pattern in tuple to be able to use Pandas' str.extract() method, which our regex_create_pandas_col() function relies on!\n",
    "\n",
    "# parse the 'true' listing ids:\n",
    "df['true_listing_ids'] = regex_create_pandas_col(df, 'listing_urls', regex_pattern)\n",
    "\n",
    "# sanity check\n",
    "\n",
    "print(f\"True listing ids:\\n{df['true_listing_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of ids:\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# 2 b) Transform ids col to int, and then object, so that data types of ids and 'true_listing_ids' cols match each other (and without the unneeded 1st decimal points (ie, the '.0' values) )\n",
    "def transform_col_to_int(df, col):\n",
    "    return df[col].astype(float).astype('Int64') # use Int64 (or int64) due to the long id values\n",
    "\n",
    "# convert col data type function\n",
    "def transform_dtype_of_col(df, col, data_type):\n",
    "    \"\"\"Convert col to specified data type\"\"\"\n",
    "    return df[col].astype(data_type)\n",
    "\n",
    "\n",
    "# 2 b) Transform ids col to object, so that data types of ids and 'true_listing_ids' cols match each other\n",
    "# convert 'ids' to integer\n",
    "df['ids'] = transform_col_to_int(df, 'ids')\n",
    "# convert 'ids' data type to object:\n",
    "df['ids'] = transform_dtype_of_col(df, 'ids', str)\n",
    "\n",
    "# sanity check that ids are now of object data type\n",
    "print(f\"Data type of ids:\\n{df['ids'].dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 c) Remove any rows in which the scraped ids do not exactly match the 'true' listing ids, thereby routing any rows that were adversely impacted by the misalignment issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data--now that misaligned data have been removed--is:\n",
      "                                            listing_urls         ids   sqft  \\\n",
      "0      https://sfbay.craigslist.org/eby/apa/d/berkele...  7401005952    NaN   \n",
      "1      https://sfbay.craigslist.org/eby/apa/d/san-ram...  7401007644  898.0   \n",
      "2      https://sfbay.craigslist.org/eby/apa/d/fremont...  7401007110  615.0   \n",
      "3      https://sfbay.craigslist.org/eby/apa/d/pleasan...  7400999282    NaN   \n",
      "4      https://sfbay.craigslist.org/eby/apa/d/oakland...  7397957717  800.0   \n",
      "...                                                  ...         ...    ...   \n",
      "23092  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7423666098    NaN   \n",
      "23093  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7423621283    NaN   \n",
      "23094  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7423549135    NaN   \n",
      "23095  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7423328327    NaN   \n",
      "23096  https://sfbay.craigslist.org/sfc/apa/d/san-fra...  7426061928    NaN   \n",
      "\n",
      "              cities prices  bedrooms bathrooms  \\\n",
      "0           Berkeley  4,450       3.0         1   \n",
      "1          Danville   2,676       1.0         1   \n",
      "2           Fremont   2,013       1.0         1   \n",
      "3            Dublin   2,850       0.0         1   \n",
      "4            Oakland  2,200       2.0         1   \n",
      "...              ...    ...       ...       ...   \n",
      "23092  San Francisco  3,300       1.0         1   \n",
      "23093  San Francisco  3,231       1.0         1   \n",
      "23094  San Francisco  3,711       1.0         1   \n",
      "23095  San Francisco  3,976       1.0         1   \n",
      "23096  San Francisco  1,095       1.0    shared   \n",
      "\n",
      "                                               attr_vars  \\\n",
      "0      cats are OK - purrr\\nflooring: wood\\nfurnished...   \n",
      "1      EV charging\\nair conditioning\\ncats are OK - p...   \n",
      "2      cats are OK - purrr\\ndogs are OK - wooof\\nfloo...   \n",
      "3      air conditioning\\ncats are OK - purrr\\ndogs ar...   \n",
      "4      application fee details: $50\\nflooring: wood\\n...   \n",
      "...                                                  ...   \n",
      "23092  EV charging\\nair conditioning\\ncats are OK - p...   \n",
      "23093  EV charging\\nair conditioning\\ncats are OK - p...   \n",
      "23094  EV charging\\nair conditioning\\ncats are OK - p...   \n",
      "23095  EV charging\\nair conditioning\\ncats are OK - p...   \n",
      "23096  flooring: wood\\nfurnished\\napartment\\nno laund...   \n",
      "\n",
      "                                         listing_descrip date_of_webcrawler  \\\n",
      "0      this a lovely townhouse 4 blocks away from ucb...         2021-10-29   \n",
      "1      b103 - beautiful spacious two bedrooms one-bat...         2021-10-29   \n",
      "2      your refuge~your retreat~your perfect home\\nth...         2021-10-29   \n",
      "3      furnished studio - great monthly rates!\\ntake ...         2021-10-29   \n",
      "4      2bedroom 1bath apt. available for moving now. ...         2021-10-29   \n",
      "...                                                  ...                ...   \n",
      "23092  video|face-time|zoom|guided tours available no...         12/29/2021   \n",
      "23093  video|face-time|zoom|guided tours available no...         12/29/2021   \n",
      "23094  video|face-time|zoom|guided tours available no...         12/29/2021   \n",
      "23095  video|face-time|zoom|guided tours available no...         12/29/2021   \n",
      "23096  single room\\n\\n21 fully furnished remodeled ro...         12/29/2021   \n",
      "\n",
      "       ...  is_furnished attached_garage detached_garage carport  \\\n",
      "0      ...             1               0               0       0   \n",
      "1      ...             0               0               0       1   \n",
      "2      ...             0               0               0       1   \n",
      "3      ...             1               0               0       0   \n",
      "4      ...             0               0               0       0   \n",
      "...    ...           ...             ...             ...     ...   \n",
      "23092  ...             0               1               0       0   \n",
      "23093  ...             0               1               0       0   \n",
      "23094  ...             0               1               0       0   \n",
      "23095  ...             0               1               0       0   \n",
      "23096  ...             1               0               0       0   \n",
      "\n",
      "       off_street_parking  no_parking  EV_charging  air_condition  no_smoking  \\\n",
      "0                       0           0            0              0           0   \n",
      "1                       0           0            1              1           1   \n",
      "2                       0           0            0              0           1   \n",
      "3                       1           0            0              1           1   \n",
      "4                       0           0            0              0           0   \n",
      "...                   ...         ...          ...            ...         ...   \n",
      "23092                   0           0            1              1           1   \n",
      "23093                   0           0            1              1           1   \n",
      "23094                   0           0            1              1           1   \n",
      "23095                   0           0            1              1           1   \n",
      "23096                   0           0            0              0           1   \n",
      "\n",
      "       true_listing_ids  \n",
      "0            7401005952  \n",
      "1            7401007644  \n",
      "2            7401007110  \n",
      "3            7400999282  \n",
      "4            7397957717  \n",
      "...                 ...  \n",
      "23092        7423666098  \n",
      "23093        7423621283  \n",
      "23094        7423549135  \n",
      "23095        7423328327  \n",
      "23096        7426061928  \n",
      "\n",
      "[8515 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2 c) Remove rows from dataframe by comparing scraped ids vs the 'true' listing ids\n",
    "def remove_rows_if_cols_not_equal(df, col1, col2):\n",
    "    \"\"\"Remove row from given DataFrame if values from col1 and col2 are not equal\"\"\"\n",
    "    df = df.loc[df[col1] == df[col2]]\n",
    "    return df\n",
    "\n",
    "# 2 c) Remove rows from dataframe by comparing scraped ids (ie, 'ids') vs the 'true' listing ids (ie, 'true_listing_ids')\n",
    "df = remove_rows_if_cols_not_equal(df, 'ids', 'true_listing_ids') \n",
    "\n",
    "# sanity check\n",
    "print(f\"Data--now that misaligned data have been removed--is:\\n{df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 a.) Rename cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apt</th>\n",
       "      <th>condo</th>\n",
       "      <th>single_fam</th>\n",
       "      <th>duplex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23092</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23093</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23094</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23095</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23096</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8515 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       apt  condo  single_fam  duplex\n",
       "0        0      0           0       0\n",
       "1        1      0           0       0\n",
       "2        1      0           0       0\n",
       "3        1      0           0       0\n",
       "4        1      0           0       0\n",
       "...    ...    ...         ...     ...\n",
       "23092    1      0           0       0\n",
       "23093    1      0           0       0\n",
       "23094    1      0           0       0\n",
       "23095    1      0           0       0\n",
       "23096    1      0           0       0\n",
       "\n",
       "[8515 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def rename_cols(df, rename_cols_dict):\n",
    "    df = df.rename(columns=rename_cols_dict)\n",
    "    return df\n",
    "\n",
    "# specify dictionary to specify what cols to rename, and vals for the renamed cols\n",
    "dict_rename_cols = {\n",
    "    'apt_type':'apt',\n",
    "    'in_law_apt_type':'in_law_apt',\n",
    "    'condo_type':'condo',\n",
    "    'townhouse_type':'townhouse',\n",
    "    'cottage_or_cabin_type':'cottage_or_cabin',\n",
    "    'single_fam_type':'single_fam',\n",
    "    'duplex_type':'duplex'    \n",
    "    }\n",
    "\n",
    "\n",
    "# 3 a) Rename cols:\n",
    "df = rename_cols(df, dict_rename_cols)\n",
    "\n",
    "# sanity check on col names:\n",
    "df[['apt', 'condo', 'single_fam', 'duplex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 b) Add the 2 new indicator columns to match the newer webcrawler specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts of the new 'flat' col:\n",
      "0.0    3061\n",
      "1.0      45\n",
      "Name: flat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3 b) Add and parse additional indicator var cols:\n",
    "# create indicator var using numpy and Pandas' str.contains() based on scraped rental listing attributes and descriptions  \n",
    "def indicator_vars_from_scraped_data(df, col_to_parse, attr_substr):\n",
    "    return pd.Series(np.where(df[col_to_parse].str.contains(attr_substr), 1, 0))\n",
    "\n",
    "\n",
    "# 'flat:\n",
    "df['flat'] = indicator_vars_from_scraped_data(df, 'attr_vars', 'flat') \n",
    "\n",
    "# land\n",
    "df['land'] = indicator_vars_from_scraped_data(df, 'attr_vars', 'land')\n",
    "\n",
    "# sanity check\n",
    "\n",
    "print(f\"Value counts of the new 'flat' col:\\n{df['flat'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 c) Move the 2 new indicator cols to the corresponding locations to match the webcrawler specs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure 'flat' has been moved to proper location:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "## Namely: move 'flat' to index just to right of 'duplex', and move 'land' to right of 'flat'\n",
    "## Ergo: let's start by looking up index location of 'duplex':\n",
    "\n",
    "# look up index location of given col\n",
    "def look_up_index_loc_of_col(df, col):\n",
    "    \"\"\" Return index location of given column, given column name\"\"\"\n",
    "    index_of_col = df.columns.get_loc(col)\n",
    "    return index_of_col\n",
    "\n",
    "# index location of 'duplex' col\n",
    "index_of_duplex = look_up_index_loc_of_col(df,  'duplex')  # look up index location for 'duplex' col)\n",
    "\n",
    "# Now, determine the new 'flat' col index location by adding 1 to the duplex index loc\n",
    "flat_new_loc = index_of_duplex + 1 # add 1 to duplex loc to determine the location where we want to move 'flat'\n",
    "\n",
    "\n",
    "# specify function to move col location for dataframe:\n",
    "def move_col_loc_for_df_dict(df, col, index_loc_to_move):\n",
    "    col = df.pop(col)  # sequester given col from each df \n",
    "    df.insert(index_loc_to_move, col.name, col)  # move location of given col within df\n",
    "    return df \n",
    "\n",
    "\n",
    "# move 'flat' col:\n",
    "df = move_col_loc_for_df_dict(df, 'flat', flat_new_loc)\n",
    "\n",
    "# sanity check\n",
    "print(f\"Ensure 'flat' has been moved to proper location:\\n{df.iloc[0, flat_new_loc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move 'land' col:\n",
    "\n",
    "# Determine 'land' col new index location--NB: Since we want 'land' 1 col to right of 'flat', simply add 1 to the flat_new_loc\n",
    "land_new_loc = flat_new_loc + 1 # add 1 to flat's (soon-to-be) new location so it is contiguous 1 col to the right\n",
    "\n",
    "\n",
    "# move 'land' col:\n",
    "df = move_col_loc_for_df_dict(df, 'land', land_new_loc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 d) Remove unneeded cols to match the webcrawler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['listing_urls', 'ids', 'sqft', 'cities', 'prices', 'bedrooms',\n",
      "       'bathrooms', 'attr_vars', 'listing_descrip', 'date_of_webcrawler',\n",
      "       'kitchen', 'date_posted', 'region', 'sub_region', 'cats_OK', 'dogs_OK',\n",
      "       'wheelchair_accessible', 'laundry_in_bldg', 'no_laundry',\n",
      "       'washer_and_dryer', 'washer_and_dryer_hookup', 'laundry_on_site',\n",
      "       'full_kitchen', 'dishwasher', 'refrigerator', 'oven', 'flooring_carpet',\n",
      "       'flooring_wood', 'flooring_tile', 'flooring_hardwood', 'flooring_other',\n",
      "       'apt', 'in_law_apt', 'condo', 'townhouse', 'cottage_or_cabin',\n",
      "       'single_fam', 'duplex', 'flat', 'land', 'is_furnished',\n",
      "       'attached_garage', 'detached_garage', 'carport', 'off_street_parking',\n",
      "       'no_parking', 'EV_charging', 'air_condition', 'no_smoking'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def remove_cols(df):\n",
    "    df = df.drop(columns=['true_listing_ids'])\n",
    "    return df\n",
    "\n",
    "df = remove_cols(df)\n",
    "\n",
    "# sanity check\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Export cleaned/transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.) Export cleaned data to one large CSV file--in the 'old_scraped_data' sub-directory within the main sfbay folder containing scraped rental listings data:\n",
    "def df_to_csv(df, direc, CSV_file_name):\n",
    "    return df.to_csv(direc + '\\\\'+ CSV_file_name, index=False)\n",
    "\n",
    "# NB: I've manually created a new sub-folder within the main scraped sfbay directory, to contain just these cleaned 'old' scraped data--ie, scraped via an older version of the webcrawler (namely: prior to the bug fix on Jan 1, 2022)\n",
    "direc_to_export = r\"D:\\\\Coding and Code projects\\\\Python\\\\craigslist_data_proj\\\\CraigslistWebScraper\\\\scraped_data\\\\sfbay\\\\old_scraped_data\"\n",
    "\n",
    "\n",
    "## export\n",
    "df_to_csv(df, direc_to_export, 'craigslist_all_sfbay_subregions_Oct_27_to_Dec_31_2021.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9245c747e8241d920f220897f12edbe786b61c4594b5fa55595ea3a9c3131f03"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 32-bit ('craigslist_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
